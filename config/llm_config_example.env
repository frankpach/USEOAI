# LLM Configuration Example
# Copy this file to .env and configure your API keys

# =============================================================================
# LLM PROVIDER ENABLE/DISABLE
# =============================================================================

# Enable/disable specific LLM providers (true/false)
LLM_ENABLE_OPENAI=true
LLM_ENABLE_ANTHROPIC=false
LLM_ENABLE_GEMINI=false

# =============================================================================
# LLM PRIORITY ORDER
# =============================================================================

# Priority order for LLM providers (comma-separated)
# First available provider in this order will be used
# Options: "openai", "anthropic", "gemini"
LLM_PRIORITY_ORDER=openai,anthropic,gemini

# =============================================================================
# API KEYS
# =============================================================================

# OpenAI API Key (required if LLM_ENABLE_OPENAI=true)
OPENAI_API_KEY=sk-your-openai-api-key-here

# Anthropic API Key (required if LLM_ENABLE_ANTHROPIC=true)
ANTHROPIC_API_KEY=sk-ant-your-anthropic-api-key-here

# Google Gemini API Key (required if LLM_ENABLE_GEMINI=true)
GOOGLE_API_KEY=your-google-api-key-here

# =============================================================================
# MODEL CONFIGURATION
# =============================================================================

# OpenAI Model (default: gpt-4)
OPENAI_MODEL=gpt-4

# Anthropic Model (default: claude-3-opus-20240229)
ANTHROPIC_MODEL=claude-3-opus-20240229

# Google Gemini Model (default: gemini-1.5-pro)
GEMINI_MODEL=gemini-1.5-pro

# =============================================================================
# REQUEST PARAMETERS
# =============================================================================

# Temperature for all LLM requests (0.0-1.0, default: 0.3)
LLM_TEMPERATURE=0.3

# Maximum tokens for responses (default: 1000)
LLM_MAX_TOKENS=1000

# Request timeout in seconds (default: 30)
LLM_TIMEOUT=30

# =============================================================================
# FALLBACK BEHAVIOR
# =============================================================================

# Enable fallback to other providers if primary fails (true/false, default: true)
LLM_ENABLE_FALLBACK=true

# Maximum retry attempts per provider (default: 3)
LLM_MAX_RETRIES=3

# Delay between retries in seconds (default: 1.0)
LLM_RETRY_DELAY=1.0

# =============================================================================
# USAGE EXAMPLES
# =============================================================================

# Example 1: Use only OpenAI
# LLM_ENABLE_OPENAI=true
# LLM_ENABLE_ANTHROPIC=false
# LLM_ENABLE_GEMINI=false
# LLM_PRIORITY_ORDER=openai

# Example 2: Use OpenAI as primary, Anthropic as fallback
# LLM_ENABLE_OPENAI=true
# LLM_ENABLE_ANTHROPIC=true
# LLM_ENABLE_GEMINI=false
# LLM_PRIORITY_ORDER=openai,anthropic

# Example 3: Use all providers with custom priority
# LLM_ENABLE_OPENAI=true
# LLM_ENABLE_ANTHROPIC=true
# LLM_ENABLE_GEMINI=true
# LLM_PRIORITY_ORDER=anthropic,openai,gemini

# Example 4: Disable all LLM providers (for testing without API costs)
# LLM_ENABLE_OPENAI=false
# LLM_ENABLE_ANTHROPIC=false
# LLM_ENABLE_GEMINI=false 